{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"boto3\");\n",
    "sc.install_pypi_package('sagemaker');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define user specific parameters\n",
    "region = 'us-west-2'\n",
    "source_bucket = 's3a://emr-lab-income-dataset/'\n",
    "sagemaker_execution_role = 'arn:aws:iam::883624334343:role/service-role/AmazonSageMaker-ExecutionRole-20190906T093404'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "sage_sdk_session = sagemaker.Session(boto_session=boto_sess)\n",
    "bucket = sage_sdk_session.default_bucket()\n",
    "\n",
    "print('A SageMaker session was initiated! You are using {} as your S3 bucket for intermediate files.'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will use the abalone data set from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Abalone).\n",
    "\n",
    "   Given is the attribute name, attribute type, the measurement unit and a\n",
    "   brief description.  The number of rings is the value to predict: either\n",
    "   as a continuous value or as a classification problem.\n",
    "\n",
    "\tName\t\t\tData Type\t\tMeas.\tDescription\n",
    "\t----\t\t\t---------\t\t-----\t-----------\n",
    "\tRings\t\t\tinteger\t\t\t\t\t+1.5 gives the age in years\n",
    "\tLength\t\t\tcontinuous\t\tmm\t\tLongest shell measurement\n",
    "\tDiameter\t\tcontinuous\t\tmm\t\tperpendicular to length\n",
    "\tHeight\t\t\tcontinuous\t\tmm\t\twith meat in shell\n",
    "\tWhole weight\tcontinuous\t\tgrams\twhole abalone\n",
    "\tShucked weight\tcontinuous\t\tgrams\tweight of meat\n",
    "\tViscera weight\tcontinuous\t\tgrams\tgut weight (after bleeding)\n",
    "\tShell weight\tcontinuous\t\tgrams\tafter being dried\n",
    "\tMale\t\t\tinteger\t\t\t1/0 \t1 encodes true, 0 false\n",
    "\tFemale\t\t\tinteger\t\t\t1/0 \t1 encodes true, 0 false\n",
    "\tInfant\t\t\tinteger\t\t\t1/0 \t1 encodes true, 0 false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull down dataset from the S3\n",
    "abaloneData = spark.read.load(source_bucket + 'clean/', format='csv', inferSchema=True, header=True)\n",
    "abaloneData.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe in to training and validation data\n",
    "trainData, testData = abaloneData.randomSplit([.75,.25])\n",
    "\n",
    "s3_train_emr = 's3a://'+ bucket + '/train/'\n",
    "s3_test_emr = 's3a://'+ bucket + '/test/'\n",
    "data_format = 'csv'\n",
    "\n",
    "#Save the data in to S3 for later training by SageMaker\n",
    "trainData.write.save(s3_train_emr, format=data_format, mode='overwrite')\n",
    "testData.write.save(s3_test_emr, format=data_format, mode='overwrite')\n",
    "\n",
    "print('Training dataset saved in {} format to {}!'.format(data_format, s3_train_emr))\n",
    "print('Testing dataset saved in {} format to {}!'.format(data_format, s3_test_emr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Machine Learning Model in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'XGBoost'\n",
    "#model = 'LinearLeaner'\n",
    "\n",
    "l2 = 1\n",
    "l1 = 1\n",
    "\n",
    "training_images = {\n",
    "    'LinearLearner': '174872318107.dkr.ecr.{}.amazonaws.com/linear-learner:latest'.format(region),\n",
    "    'XGBoost': '433757028032.dkr.ecr.{}.amazonaws.com/xgboost:latest'.format(region)\n",
    "}\n",
    "\n",
    "linear_hyperparams = {\n",
    "    'feature_dim':len(abaloneData.columns)-1,\n",
    "    'predictor_type': 'regressor',\n",
    "    'loss': 'squared_loss',\n",
    "    'wd': l2,\n",
    "    'l1': l1\n",
    "}\n",
    "\n",
    "xgboost_hyperparams = {\n",
    "    'num_round':100,\n",
    "    'lambda': l2,\n",
    "    'objective': 'reg:linear',\n",
    "    'alpha': l1\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    'LinearLearner': linear_hyperparams,\n",
    "    'XGBoost': xgboost_hyperparams\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_name=training_images[model],\n",
    "    role=sagemaker_execution_role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.m5.large',\n",
    "    sagemaker_session=sage_sdk_session, \n",
    "    hyperparameters=hyperparams[model]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO The first time we write to S3 it creates a _SUCCESS file. This throws an error when calling SageMaker\n",
    "#If you specify the file directly will it work? YES\n",
    "s3_train = s3_train_emr.replace('s3a://', 's3://')\n",
    "train_channel = sagemaker.session.s3_input(s3_train, content_type='text/csv')\n",
    "estimator.fit({'train': train_channel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Results From SageMaker Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did our algorithm perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.large',\n",
    "    strategy = 'MultiRecord',\n",
    "    output_path = s3_inference,\n",
    "    assemble_with= 'Line',\n",
    "    accept='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_inference = s3_train.replace('train', 'inference')\n",
    "\n",
    "transformer.transform(\n",
    "    data=s3_test,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line',\n",
    "    input_filter='$[1:]',\n",
    "    join_source='Input',\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull down the inference data from S3\n",
    "inference_data = spark.read.load(s3_inference, format='csv', inferSchema=True, header=False)\n",
    "inference_data.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rings = inference_data.schema.names[0]\n",
    "predicted_rings = inference_data.schema.names[-1]\n",
    "\n",
    "SQL_RMSE = 'SELECT SQRT(AVG(POWER({}-{}, 2))) AS RMSE FROM inference'.format(rings, predicted_rings)\n",
    "\n",
    "inference_data.registerTempTable(\"inference\")\n",
    "test = spark.sql(SQL_RMSE)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "Congratulations! You processed data in Apache Spark on EMR and trained and deployed a machine learning model in Amazon SageMaker! Feel free to try different combinations of models and hyperparameters to see if you can reduce your model's RMSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
