{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"boto3\")\n",
    "sc.install_pypi_package('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define user specific parameters\n",
    "region = 'us-west-2'\n",
    "source_bucket = 's3a://emr-lab-income-dataset/'\n",
    "sagemaker_execution_role = 'arn:aws:iam::883624334343:role/service-role/AmazonSageMaker-ExecutionRole-20190906T093404'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region = 'us-west-2'\n",
    "\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "sage_sdk_session = sagemaker.Session(boto_session=boto_sess)\n",
    "bucket = sage_sdk_session.default_bucket()\n",
    "\n",
    "print('A SageMaker session was initiated! You are using {} as your S3 bucket for intermediate files.'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will use the abalone data set from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Abalone).\n",
    "\n",
    "   Given is the attribute name, attribute type, the measurement unit and a\n",
    "   brief description.  The number of rings is the value to predict: either\n",
    "   as a continuous value or as a classification problem.\n",
    "\n",
    "\tName\t\t\tData Type\t\tMeas.\tDescription\n",
    "\t----\t\t\t---------\t\t-----\t-----------\n",
    "\tRings\t\t\tinteger\t\t\t\t\t+1.5 gives the age in years\n",
    "\tLength\t\t\tcontinuous\t\tmm\t\tLongest shell measurement\n",
    "\tDiameter\t\tcontinuous\t\tmm\t\tperpendicular to length\n",
    "\tHeight\t\t\tcontinuous\t\tmm\t\twith meat in shell\n",
    "\tWhole weight\tcontinuous\t\tgrams\twhole abalone\n",
    "\tShucked weight\tcontinuous\t\tgrams\tweight of meat\n",
    "\tViscera weight\tcontinuous\t\tgrams\tgut weight (after bleeding)\n",
    "\tShell weight\tcontinuous\t\tgrams\tafter being dried\n",
    "\tMale\t\t\tinteger\t\t\t1/0 \t1 encodes true, 0 false\n",
    "\tFemale\t\t\tinteger\t\t\t1/0 \t1 encodes true, 0 false\n",
    "\tInfant\t\t\tinteger\t\t\t1/0 \t1 encodes true, 0 false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull down dataset from the S3\n",
    "abaloneData = spark.read.load(source_bucket + 'clean/', format='csv', inferSchema=True, header=True)\n",
    "abaloneData.printSchema()\n",
    "abaloneData.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe in to training and validation data\n",
    "trainData, testData = abaloneData.randomSplit([.8,.2])\n",
    "\n",
    "s3_train_emr = source_bucket + 'train/'\n",
    "s3_test_emr = source_bucket + 'test/'\n",
    "data_format = 'csv'\n",
    "\n",
    "#Save the data in to S3 for later training by SageMaker\n",
    "trainData.write.save(s3_train_emr, format=data_format, mode='overwrite')\n",
    "testData.write.save(s3_test_emr, format=data_format, mode='overwrite')\n",
    "\n",
    "print('Train dataset saved in {} format to {}!'.format(data_format, s3_train_emr))\n",
    "print('Test dataset saved in {} format to {}!'.format(data_format, s3_test_emr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Hosting a Machine Learning Model in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = 1\n",
    "\n",
    "training_images = {'LinearLearner': '174872318107.dkr.ecr.{}.amazonaws.com/linear-learner:1'.format(region),\n",
    "                  'XGBoost': '433757028032.dkr.ecr.{}.amazonaws.com/xgboost:latest'.format(region)}\n",
    "\n",
    "linear_hyperparams = {'feature_dim':len(abaloneData.columns)-1,\n",
    "                      'predictor_type': 'regressor',\n",
    "                      'loss': 'squared_loss',\n",
    "                      'wd': l2}\n",
    "\n",
    "xg_boost_hyperparams = {'num_round':100,\n",
    "                        'lambda': l2,\n",
    "                        'objective': 'reg:linear'}\n",
    "\n",
    "hyperparams = {'LinearLearner': linear_hyperparams,\n",
    "                  'XGBoost': xg_boost_hyperparams}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "            image_name=training_images['XGBoost]',\n",
    "            role=sagemaker_execution_role, \n",
    "            train_instance_count=1, \n",
    "            train_instance_type='ml.m5.large',\n",
    "            output_path=None, \n",
    "            output_kms_key=None, \n",
    "            base_job_name=None, \n",
    "            sagemaker_session=sage_sdk_session, \n",
    "            hyperparameters=hyperparams['XGBoost'], \n",
    "            train_use_spot_instances=False, \n",
    "            train_max_wait=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = s3_train_emr.replace('s3a://', 's3://')\n",
    "train_channel = sagemaker.session.s3_input(s3_train, content_type='text/csv')\n",
    "estimator.fit({'train': train_channel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', \n",
    "                             serializer=csv_serializer, content_type=CONTENT_TYPE_CSV,\n",
    "                            deserializer=json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did our algorithm perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = '0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155, 1, 0, 0'\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the lab, take down the SageMaker resources that were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
